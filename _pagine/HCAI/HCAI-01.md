---
layout: page
title: Human-Centred Artificial Intelligence 1
args: (Human-Centred AI Framework, Design metaphors )
permalink: /HCAI-01/

---

Argomenti
- [1. Human-Centred Artificial Intelligence](#1-Human-Centred-Artificial-Intelligence)
- [2. Human-Centred Artificial Intelligence Framework](#2-Human-Centred-AI-Framework)
- [3. Design metaphors](#3-Design-metaphors )

## Human-Centred Artificial Intelligence
La Human-Centred Artificial Intelligence si propone di sviluppare sistemi che non sostituiscano l’uomo, ma lo amplifichino, potenzino e supportino, mantenendo un equilibrio tra automazione e controllo umano. 

Questo approccio nasce come risposta alla visione tradizionale dell’AI, focalizzata sull’autonomia delle macchine, e pone al centro valori umani quali diritti, giustizia e dignità, oltre a obiettivi individuali come creatività, responsabilità e connessioni sociali. Il design di HCAI si basa su metodologie di user experience, coinvolgendo stakeholder e garantendo affidabilità, sicurezza e trasparenza. 

HCAI propone una visione del futuro tecnologico basata su controllo umano, valori etici e amplificazione delle capacità individuali, mentre gli scettici ritengono che il progresso continuerà a dipendere soprattutto da algoritmi sempre più avanzati. La sfida è dimostrare che un approccio centrato sull’uomo può portare benefici concreti, sostenendo creatività, responsabilità e connessioni sociali, in un contesto di innovazione che, se guidata con attenzione, può migliorare la vita di molti.


### Artificial Intelligence

L’AI è l’insieme di artefatti capaci di percepire l’ambiente e agire per raggiungere obiettivi. Il futuro dell’AI passa attraverso un equilibrio tra innovazione tecnologica e responsabilità sociale, per garantire benefici diffusi e ridurre i rischi.


### Responsable AI

La Responsable AI è assunzione di responsabilità per il potere che l’AI porta. Non è l’AI ad essere responsabile, ma chi la progetta e la utilizza. Ciò implica considerare le conseguenze sociali e culturali, progettando sistemi secondo principi etici e valori umani. Si parla di value pluralism, includendo aspetti come equità, sicurezza, benessere e solidarietà, e di tre approcci etici: Ethics in Design, Ethics by Design ed Ethics for Designers, che spaziano dalla regolamentazione alla integrazione del ragionamento etico negli algoritmi.

## Human-Centred AI Framework

La Human-Centred AI Framework evidenzia l’importanza di progettare sistemi che combinino alti livelli di automazione con alti livelli di controllo umano. L’obiettivo è superare la visione tradizionale che vede più autonomia come meno controllo umano, proponendo un approccio che valorizzi la creatività, la responsabilità e le connessioni sociali. Questo è cruciale soprattutto in ambiti complessi come trasporti, sanità, finanza e difesa, dove errori possono avere conseguenze gravi. Per garantire sicurezza e affidabilità, si suggeriscono strategie come controlli di attivazione e override, audit trail per analizzare le prestazioni e una progettazione basata sull’umiltà, consapevole dei possibili fallimenti.

Il Framework critica i modelli a una dimensione dei livelli di automazione, come quelli applicati alle auto autonome, perché non considerano adeguatamente il ruolo umano. Viene sottolineato che gli esseri umani devono monitorare i sistemi autonomi e sapere cosa stanno facendo, per evitare incidenti. Allo stesso tempo, si riconosce che l’automazione può ridurre errori umani, ma richiede design vigilante e controlli ben progettati per essere efficace. Il quadrante desiderato nel framework bidimensionale HCAI è spesso quello in alto a destra, che rappresenta alti livelli di controllo umano e alti livelli di automazione, necessari soprattutto per compiti complessi e poco compresi.

Separare i livelli di automazione dai livelli di controllo umano rende chiaro che è possibile ottenere entrambi con una buona progettazione, migliorando così la sicurezza, l'affidabilità e la fiducia nei sistemi automatizzati.


##  Design metaphors 

Due grandi visioni hanno guidato l’evoluzione dell’AI: la scienza dell’emulazione, che mira a far svolgere alle macchine compiti tipici degli esseri umani, e l’innovazione orientata ai supertool, cioè strumenti avanzati che potenziano il lavoro delle persone senza sostituirle.

La prima visione – la **science goal** – studia i processi percettivi, cognitivi e motori umani per costruire sistemi in grado di raggiungere o superare le capacità umane: riconoscimento di immagini, linguaggio, robotica, comuni­sense reasoning, fino alle ambizioni più speculative dell’AGI. Questa prospettiva si è sviluppata passando dai sistemi simbolici storici alle odierne tecniche di machine learning e deep learning, che grazie a reti neurali, GAN, CNN e modelli di fondazione hanno reso possibili risultati prima impensabili. Tuttavia, rimangono limiti reali: molte applicazioni funzionano bene in laboratorio ma falliscono in contesti complessi e imprevedibili. Molti critici osservano come diversi successi attribuiti all’AI – come Deep Blue – siano in realtà frutto di soluzioni ingegneristiche tradizionali più che di “intelligenza” nel senso umano del termine.

La seconda visione – l’ **innovation goal** – ha invece un taglio molto più pratico e orientato alle persone. Qui l’obiettivo è progettare strumenti, applicazioni e servizi che si integrano nella vita quotidiana e professionale: smartphone, sistemi di navigazione, altoparlanti intelligenti, servizi di teleconferenza, software per la collaborazione. Queste tecnologie partono spesso da competenze scientifiche avanzate (per esempio il riconoscimento vocale o la traduzione automatica), ma vengono trasformate in prodotti utili grazie ai metodi dell’HCI: osservazione degli utenti, iterazione, test, semplicità, trasparenza, controllo umano. Nell’innovazione goal, infatti, si riconosce che gli utenti preferiscono sistemi comprensibili, prevedibili e controllabili, e che l’eccesso di automazione può essere altrettanto rischioso quanto un eccesso di controllo manuale.

### Agenti Intelligenti vs Supertool

Le **metafore progettuali**, sono utili per interpretare diversamente il ruolo delle macchine. Da un lato emergono le metafore degli **agenti intelligenti**: sistemi autonomi che agiscono, prendono decisioni, apprendono, comunicano. Dall’altro lato si affermano le metafore dei **supertool**: strumenti potenti che amplificano le competenze delle persone senza pretendere di sostituirle o imitarle. Storicamente, la retorica dell’intelligenza autonoma – la “thinking machine” – ha generato grandi aspettative, ma anche fraintendimenti e delusioni. La visione alternativa, sviluppata fin dagli anni Sessanta con la “man-computer symbiosis” di Licklider, invita invece a concepire computer e persone come partner complementari: i sistemi automatizzano ciò che è ripetitivo e complesso, mentre gli esseri umani mantengono intuizione, giudizio ed esperienza.

Molti sistemi moderni per superare la contrapposizione combinino entrambe le prospettive. Un esempio è il sistema di navigazione GPS: usa algoritmi sofisticati, ma al tempo stesso offre all’utente un’interfaccia che lascia la decisione finale alla persona. Lo stesso vale per le fotocamere digitali, che integrano funzioni AI ma lasciano all’utente la scelta di inquadrature e modalità creative. Sono esempi di “combined design”: tecnologie che sfruttano algoritmi avanzati ma allo stesso tempo rispettano il bisogno umano di controllo.

## Teammates vs Tele-bots

Una certa prospettiva dell'evoluzione dei sistemi di intelligenza artificiale tende a vedere i robot come **“teammates”**, ossia come compagni di squadra. La ricerca psicologica ha mostrato che le persone tendono a rispondere ai computer come se fossero esseri sociali quando questi vengono progettati in modo umanoide. Questo ha spinto molti ricercatori a sviluppare robot che imitano l’aspetto e il comportamento umano. Tuttavia, diversi studiosi mettono in guardia contro questa scelta: i robot non sono persone, non hanno responsabilità morale o legale, e rischiano di creare aspettative sbagliate negli utenti, portando a uso improprio, fiducia eccessiva o delusione. La metafora del “teammate” può quindi essere fuorviante: invece di imitare l’essere umano, è spesso più efficace valorizzare le capacità uniche dei computer – precisione, sensori avanzati, database enormi, capacità di calcolo – lasciando alle persone il ruolo creativo e decisionale.

Esiste una  metafora alternativa: quella dei **tele-bots**, dispositivi controllati a distanza da esseri umani. Questa visione riconosce la realtà operativa di molti robot in ambito militare, medico, industriale e civile: strumenti avanzati, spesso potenti e sofisticati, ma comandati da persone responsabili. È l'approccio degli aerei militari a pilotaggio remoto, dei robot per chirurgia assistita o dei veicoli usati per l’esplorazione e la manutenzione in ambienti pericolosi. In tutti questi casi, la tecnologia non rimpiazza l'essere umano: lo potenzia.

La via più promettente per l’AI non è scegliere tra emulazione o innovazione, ma combinare entrambe le prospettive all’interno del paradigma HCAI: sistemi sicuri, affidabili, trasparenti e progettati per responsabilizzare le persone, non per sostituirle. L’obiettivo finale è costruire tecnologie che amplifichino l’efficacia umana, rispettando differenze fondamentali tra esseri umani e computer, e valorizzando ciò che le macchine possono fare meglio senza mai dimenticare che responsabilità, creatività, giudizio e valori rimangono umani.

## Assured Autonomy vs Control Center 

Un concetto che negli anni si è diffuso nel linguaggio comune fino a evocare, spesso in modo improprio, l’immagine di macchine completamente indipendenti dagli esseri umani. Le slide chiariscono come questa visione sia vaga e fuorviante. Secondo la definizione del US Defense Science Board, un sistema può dirsi realmente autonomo solo se è in grado di selezionare tra diversi corsi d’azione per raggiungere un obiettivo sulla base della propria conoscenza del mondo, di sé e della situazione. Tuttavia, la stessa istituzione sottolinea l’elemento più importante: anche i sistemi autonomi sono sempre supervisionati da esseri umani, e i limiti della loro autonomia sono codificati nel software stesso. L’autonomia, insomma, non è mai totale, né tantomeno una soluzione magica ai problemi.

I sistemi autonomi operano in contesti sociali e organizzativi complessi in cui persone e macchine sono inevitabilmente interdipendenti. Per questo motivo, la prospettiva del combined design suggerisce che alcune funzioni possano essere rese autonome solo se rimangono comprensibili, prevedibili e controllabili, mentre le funzioni cruciali, non ancora affidabili o ad alto impatto, devono rimanere sotto controllo diretto degli utilizzatori. Si ricorda inoltre come vari ricercatori abbiano evidenziato i problemi dell’autonomia: difficoltà di coordinamento tra sistemi automatici e persone, aumento inatteso del carico di lavoro umano dovuto alla necessità di monitorare costantemente la macchina, calo di vigilanza nei periodi di inattività e riduzione delle competenze quando gli operatori devono intervenire improvvisamente. Sono paradossi noti da decenni, ma ancora oggi estremamente attuali.

Non mancano posizioni più critiche: alcuni autori, come Peter Hancock, sostengono che la diffusione crescente di dispositivi autonomi sia “inevitabile” e che ciò possa essere pericoloso, arrivando a invocare restrizioni forti o l’eliminazione di tali tecnologie. Altri studiosi si concentrano sul tema della responsabilità: poiché la responsabilità legale e morale ricade sempre sulle persone e sulle organizzazioni, la progettazione di sistemi autonomi deve essere affrontata con grande attenzione. In questo contesto nascono concetti come **Assured Autonomy**, sostenuto da istituzioni come il Johns Hopkins Institute for Assured Autonomy, che punta sulla fiducia, sulla trasparenza, sulle verifiche continue e su un chiaro quadro politico. Tuttavia, le slide mettono in guardia: l’idea di “autonomia affidabile” rischia di illudere gli sviluppatori, spingendoli a credere che sia possibile costruire sistemi completamente sicuri con una supervisione umana minima, cosa ancora lontana dalla realtà.

Per questo motivo, viene introdotto il concetto più pragmatico di **Supervised Autonomy**. Qui l’automazione rimane, ma sempre sotto il controllo visibile e strutturato di operatori umani, attraverso interfacce, pannelli di controllo e veri e propri centri di supervisione. Questo approccio si basa sulla consapevolezza che l’autonomia può essere utile e persino necessaria, ma deve sempre essere accompagnata da persone in grado di intervenire rapidamente e con piena conoscenza della situazione. Un esempio significativo è la “parallel autonomy” immaginata da Daniela Rus: durante la guida, l’utente mantiene il controllo del veicolo, mentre l’AI interviene solo per prevenire incidenti, integrandosi con sensibilità nel comportamento del conducente.


## Social Robots vs Active Appliance

Un ulteriore tema riguarda il contrasto tra **robot sociali** e **active appliances**. 

I robot sociali sono quelli che imitano forme, movimenti o comportamenti umani: umanoidi, androidi, robot “espressivi”. La fascinazione per questa idea è antica, affondando le radici nei miti greci, negli automi del Settecento, nel Golem e in Frankenstein, fino ai robot della narrativa moderna. Non sorprende quindi che progetti come Asimo, Sophia o Jibo abbiano attirato l’attenzione del grande pubblico. Tuttavia, la storia recente ha mostrato che, al di là dell’intrattenimento, questi robot hanno avuto poco o nessun successo commerciale. Spesso non rispondono a bisogni reali, risultano complessi da mantenere e creano aspettative non realistiche negli utenti. Le aziende che li hanno sviluppati hanno trovato un pubblico curioso, ma non un mercato sostenibile.

Al contrario, i robot non antropomorfi — o più propriamente gli active appliances — hanno trovato grande diffusione. Gli esempi includono robot aspirapolvere come Roomba, sistemi di controllo domestico, dispositivi di sicurezza autonoma, elettrodomestici intelligenti, macchine per la cura della casa e del giardino, e persino apparecchiature mediche o fitness dotate di AI. Questi dispositivi non cercano di imitare l’essere umano, ma svolgono bene un compito preciso, spesso con un’interfaccia semplice e con comportamenti prevedibili. Nelle case e negli uffici, le persone non desiderano compagni antropomorfi: preferiscono strumenti affidabili e facili da gestire.

Un’altra categoria crescente è quella dei robot animali, come AIBO, PARO o altri robot-compagno. In questo caso, l’imitazione non mira a sostituire l’essere umano, ma a offrire comfort, compagnia o supporto terapeutico, soprattutto per anziani e bambini. Questi dispositivi rispondono a bisogni concreti e realistici, e per questo stanno incontrando un’accoglienza più positiva rispetto ai robot umanoidi.

Sistemi come Alexa, Siri o i chatbot nei servizi clienti, sebbene non rientrino nella definizione di robot sociali, rappresentano un enorme successo commerciale e mostrano come le persone preferiscano tecnologie utili piuttosto che “umane”. Anche qui emergono concetti fondamentali dell’HCAI: non bisogna cercare di ingannare gli utenti, ma fornire strumenti chiari, veloci e pratici che facilitino il raggiungimento degli obiettivi.


### Conclusione

Le quattro coppie di metafore — agenti intelligenti vs supertool, teammate vs tele-bots, autonomia garantita vs centri di controllo, social robots vs active appliances — offrono prospettive complementari che devono essere integrate, non contrapposte. I progetti più riusciti combinano automazione e controllo umano, sfruttano l’AI dove funziona bene ma lasciano all’utente la responsabilità e le decisioni più importanti. La sfida del futuro sarà mantenere un equilibrio tra ambizione tecnologica e realtà pratica, tra ciò che la tecnologia promette e ciò che gli utenti realmente desiderano.
