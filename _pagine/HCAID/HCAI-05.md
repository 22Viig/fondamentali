---
layout: page
title: Human-Centred Artificial Intelligence 5
args: (Algorithmic Fairness)
permalink: /HCAI-05/

---

Argomenti
- [1. Algorithmic Fairness](#Algorithmic-Fairness)
- [2. Conclusioni](#Conclusioni)


## Algorithmic Fairness

L’**Algorithmic Fairness** è un argomento oggi centrale nell’ambito dell’intelligenza artificiale e del machine learning, soprattutto perché sistemi algoritmici vengono sempre più coinvolti in decisioni che influenzano la vita delle persone: dall’accesso al credito ai processi giudiziari, dalla selezione del personale alla moderazione dei contenuti online. Le slide cercano di chiarire perché la fairness sia necessaria, quali siano le cause dei bias algoritmici, come si definisca la correttezza di un modello e quali metodi esistano per rilevarne e mitigarne le discriminazioni.

La prima domanda affrontata è: perché abbiamo bisogno di parlare di equità algoritmica? All’apparenza, un sistema che elabora solo dati numerici dovrebbe evitare discriminazioni, poiché non ha emozioni, pregiudizi o intenzioni. È frequente sentir dire che, poiché l’algoritmo utilizza unicamente elementi “oggettivi”, non può discriminare. Ma questa è una conclusione fallace. Le persone, nella loro presa di decisione, combinano componenti oggettive e soggettive; i sistemi algoritmici, invece, dipendono completamente dai dati che ricevono. Se i dati includono bias—come storici pregiudizi umani o misurazioni difettose—l’algoritmo non solo li replicherà, ma rischierà di amplificarli. Le slide ricordano alcuni casi ormai celebri: nel 2015, una ricerca su Google Immagini mostrava acconciature “professionali” associate principalmente a donne bianche e acconciature “non professionali” a donne nere, rivelando la presenza di stereotipi nei risultati di ricerca. Ancora più grave, sempre nel 2015, Google Photos etichettò come “gorilla” due persone nere, costringendo Google a rimuovere del tutto alcune categorie di classificazione pur senza riuscire, a distanza di anni, a risolvere il problema alla radice. Situazioni simili si sono verificate con Google Vision API nel 2020, dove immagini simili venivano etichettate diversamente a seconda del colore della pelle. Questi esempi dimostrano che l’AI non è “razzista” in sé, ma apprende le distorsioni presenti nei dati utilizzati per addestrarla.

Un altro caso emblematico, su cui le slide si soffermano, riguarda il sistema COMPAS, utilizzato negli Stati Uniti per stimare la probabilità di recidiva criminale. Un’analisi condotta da ProPublica nel 2016 evidenziò che, pur avendo un’accuratezza complessiva simile tra bianchi e neri, l’algoritmo commetteva errori in modo sistematicamente diverso: era più incline a classificare come ad alto rischio individui neri che non avrebbero commesso reati, mentre tendeva a giudicare a basso rischio individui bianchi che invece sarebbero recidivati. Il problema, quindi, non riguardava l’accuratezza generale, quanto la modalità con cui gli errori erano distribuiti tra gruppi diversi. La Corte Suprema del Wisconsin stabilì che i giudici potevano utilizzare le valutazioni di COMPAS, ma solo a determinate condizioni: non per determinare la durata della pena, bensì come supporto alla valutazione del rischio, e sempre accompagnate da chiari avvisi sulle limitazioni del sistema. 

Le linee guida europee sull’AI affidabile (2019) identificano la fairness come un requisito essenziale: è un concetto sociale e culturale, che cambia nel tempo e nei contesti. La fairness implica che le valutazioni siano proporzionate e non discriminatorie. In contrapposizione, il termine bias indica un errore sistematico che colpisce gruppi diversi in modo differenziale. È importante notare che un sistema può essere biased ma non necessariamente unfair, come nel caso di un algoritmo che utilizza esplicitamente informazioni demografiche per aiutare gruppi storicamente svantaggiati: qui la “distorsione” è intenzionale e finalizzata ad aumentare l’equità. Allo stesso modo, un algoritmo può essere formalmente unbiased ma comunque unfair, se utilizza attributi proxy (per esempio il CAP di residenza) che consentono di dedurre informazioni sensibili.

Quali sono cause principali dell’iniquità nei sistemi di machine learning?  Una delle più importanti risiede nei dataset: se i dati usati per addestrare il modello derivano da decisioni umane già condizionate da pregiudizi, misurazioni sbagliate o campioni non rappresentativi, l’algoritmo imparerà questi pattern distorti. Un dataset può contenere bias impliciti nelle misure, nei valori mancanti o nei criteri di selezione dei soggetti. Inoltre, molte funzioni obiettivo degli algoritmi mirano a minimizzare l’errore totale, favorendo i gruppi più rappresentati e trascurando le minoranze: una scelta matematica che ha però conseguenze sociali significative. Infine, l’esistenza di attributi proxy—dati apparentemente innocui, come cognome o indirizzo, ma strettamente correlati a informazioni sensibili—permette ai modelli di aggirare il divieto di usare razza o genere, perpetuando di fatto discriminazioni indirette.

I criteri di fairness sono categorie concettuali che stabiliscono quando un algoritmo può essere considerato “giusto”.Si distinguono la fairness individuale e la fairness di gruppo. La prima prevede che individui simili vengano trattati allo stesso modo; la seconda richiede che le statistiche di performance siano simili tra gruppi diversi. Le teorie legali introducono concetti come disparate impact, cioè discriminazione indiretta, che si verifica quando un modello danneggia sistematicamente un gruppo pur trattando tutti allo stesso modo; e disparate treatment, cioè discriminazione diretta, quando il modello utilizza esplicitamente attributi sensibili per prendere decisioni. Una terza categoria, il disparate mistreatment, si focalizza non sulle predizioni corrette, ma sul tasso di errori: se un algoritmo sbaglia più spesso con un gruppo rispetto a un altro, può essere ritenuto ingiusto.

Accanto ai criteri concettuali esistono delle metriche per individuare e misurare la presenza di bias. Alcune misurano la parità tra le percentuali di classificazione positiva nei diversi gruppi, come il disparate impact o la statistical parity. Tuttavia, queste metriche possono penalizzare modelli molto accurati se i gruppi hanno tassi di base diversi. Per questo sono stati introdotti criteri più raffinati come equalized odds—che richiede che i tassi di veri positivi e falsi positivi siano simili tra gruppi—oppure equal opportunity, che si concentra solo sui veri positivi. Altre metriche, come overall accuracy equality o treatment equality, considerano rispettivamente la parità dell’accuratezza complessiva o il rapporto tra errori. Ogni metrica evidenzia un diverso ideale di equità, e non esiste un criterio universale: in molti casi, soddisfarne uno significa violarne un altro.

Esistono strategie di bias mitigation, cioè strumenti tecnici con cui ricercatori e ingegneri cercano di rendere i sistemi più equi. Questi approcci possono intervenire in tre fasi distinte del processo di machine learning.
I pre‑processing modificano i dati prima dell’addestramento, ad esempio riscrivendo alcune etichette vicino alla frontiera decisionale o bilanciando i pesi degli esempi. Altri metodi rimuovono gli attributi proxy o trasformano i dati in rappresentazioni più neutrali.
Gli in‑processing intervengono durante l’addestramento, integrando nella funzione obiettivo termini di regolarizzazione che forzano il modello a rispettare un criterio di fairness. Adversarial debiasing, ad esempio, addestra un classificatore insieme a un discriminatore che tenta di indovinare il gruppo sensibile: il classificatore è penalizzato se il discriminatore riesce, forzandolo a produrre rappresentazioni più eque.
Infine, i post‑processing operano sul modello già addestrato: modificano le soglie di decisione per gruppi diversi o applicano regole correttive come l’equalized odds post‑processing. Sono molto utili quando non si ha accesso al modello originale o ai dati di addestramento.

### Conclusioni

La fairness algoritmica è un campo multidisciplinare che richiede attenzione tecnica, consapevolezza sociale e dialogo etico. La necessità di modelli equi non è solo un’esigenza legale o regolatoria, ma una condizione per sviluppare sistemi che rispettino la dignità e i diritti delle persone. Gli esempi analizzati evidenziano che l’AI non è “neutrale”: senza interventi mirati, rischia di amplificare i pregiudizi pre‑esistenti. Per questo la fairness non è un obiettivo marginale, ma un requisito essenziale di qualunque tecnologia che voglia essere considerata affidabile e giusta.