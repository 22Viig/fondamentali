---
layout: page
title: Human-Centred Artificial Intelligence 2
args: (Etica e pratica, Sistemi affidabili, Safety Culture)
permalink: /HCAI-02/

---

Argomenti
- [1. Etica e Pratica](#1-Etica-e-Pratica)
- [2. Sistemi affidabili](#2-Sistemi-affidabili)
- [3. Safety Culture](#3-Safety-Culture)
- [4. Conclusione](#4-Conclusione)

### Etica e Pratica

Entriamo nel cuore di uno dei temi più delicati dell’Intelligenza Artificiale centrata sull’uomo: come trasformare i principi etici in pratiche reali, attraverso strutture di governance, metodi ingegneristici e processi di verifica che rendano i sistemi affidabili, sicuri e allineati ai valori umani. È un passaggio cruciale, perché da un lato l’AI promette benefici enormi — dal miglioramento delle diagnosi mediche alla protezione delle specie in pericolo, fino alla prevenzione dei crimini informatici — ma dall’altro porta con sé rischi significativi, come discriminazioni, violazioni della privacy, attacchi avversariali e perfino timori di comportamenti fuori controllo.

Per affrontare questa complessità, l’HCAI si fonda su un’idea semplice ma potentissima: **mettere l’essere umano al centro**, non solo come destinatario finale, ma come vero e proprio protagonista del processo di progettazione e valutazione. La tecnologia non è più considerata come un’entità autonoma, bensì come uno strumento che amplifica le capacità umane e deve quindi rispettare i loro bisogni, valori e limiti cognitivi. Questo significa realizzare sistemi che sostengano la creatività, favoriscano la partecipazione sociale, chiariscano ruoli e responsabilità, e soprattutto potenzino l’autonomia dell’utente invece di ridurla.

Gli strumenti digitali che usiamo quotidianamente — dalle interfacce grafiche ai social media, dagli e‑commerce alle app mobile — siano esempi di tecnologie che già supportano molti principi HCAI. Ma applicare questi principi con rigore richiede metodologie partecipative: coinvolgere gli stakeholder durante la progettazione, testare l’esperienza utente in modo iterativo, valutare non solo le prestazioni degli algoritmi ma anche la soddisfazione e l’efficacia dell’utente. È un cambio di paradigma: la prestazione dell’AI non è misurata solo in termini matematici, ma anche rispetto all’impatto pratico sulla vita delle persone.

Allo stesso tempo, esiste un ostacolo considerevole: la complessità crescente dei sistemi AI. A differenza delle applicazioni tradizionali, un sistema HCAI è costruito da componenti cambiano continuamente — librerie, dataset, modelli, microservizi — rendendo difficile garantire coerenza, sicurezza e trasparenza. Non bastano più i test tradizionali sul singolo modulo: servono meccanismi sociali e organizzativi di controllo, come forme di revisione indipendente, analisi dei quasi‑incidenti e valutazioni periodiche che includano anche il contesto sociale e normativo.

Per colmare questo divario tra etica e pratica, viene proposto un **modello a quattro livelli di governance**. Al livello più interno troviamo i **team di ingegneria**, responsabili del codice, del testing e delle buone pratiche tecniche. Attorno a loro c’è l’**organizzazione aziendale**, che deve promuovere una vera cultura della sicurezza: non bastano procedure astratte, serve un atteggiamento diffuso orientato alla responsabilità, alla prevenzione degli errori e alla trasparenza. Il terzo livello è quello dell’**oversight indipendente**: gruppi esterni, magari di settore, che monitorano più organizzazioni e diffondono le buone pratiche. Infine c’è la **regolamentazione pubblica**, che rappresenta il livello più ampio, quello orientato al bene comune. Qui non si tratta di limitare l’innovazione, ma di creare un quadro stabile e sicuro in cui essa possa svilupparsi senza danneggiare la società.

### Sistemi affidabili

Poiché l’AI non è programmata come il software tradizionale ma “addestrata”, le pratiche ingegneristiche devono adattarsi. L’intero ciclo di vita si lega ai dati: raccolta, pulizia, etichettatura, analisi, valutazione delle anomalie e dei bias. Il modello a cascata, troppo rigido, lascia spazio a metodi lean e agili, in cui si lavora per iterazioni brevi, a stretto contatto con gli utenti, testando prototipi, adattando soluzioni e imparando dai fallimenti precoci. In questo approccio non conta solo far funzionare un modello, ma verificarne rischi, equità, comprensibilità e coerenza nel tempo.

Il testing dell’AI richieda nuove categorie: test basati su casi, test differenziali, test metamorfici, test dell’esperienza utente e perfino test condotti da “red team” che cercano deliberatamente di violare o ingannare il sistema, come avviene in ambito militare o nella sicurezza informatica. Tutti questi strumenti servono a individuare debolezze che potrebbero portare non solo a errori tecnici, ma a danni sociali.

Gli algoritmi possono perpetuare o amplificare discriminazioni già presenti nei dati, **bias**, come mostrato da moltissimi esempi negli anni: dai modelli di credito alle procedure di assunzione, dalle immagini online alle valutazioni scolastiche. Le slide ricordano le tre forme principali di bias — pre‑esistenti, tecnici ed emergenti — e sottolineano come la mancanza di diversità nei team di sviluppo aggravi il problema. Per affrontarlo servono analisi rigorose, dataset inclusivi, criteri di equità e una chiara consapevolezza della responsabilità sociale di chi sviluppa l’AI.

La comprensibilità delle decisioni delle macchine è diventata un requisito non solo etico ma anche legale, richiamando il “diritto alla spiegazione” previsto dal GDPR europeo. Nel panorama attuale, molte tecniche di spiegabilità si basano su spiegazioni post‑hoc, realizzate cioè dopo che l’algoritmo ha già prodotto la decisione. Esempi comuni includono sistemi e‑commerce che giustificano raccomandazioni sulla base di acquisti passati. Nonostante siano diffuse, queste tecniche hanno limiti: spesso rispondono solo a posteriori e derivano da reti neurali opache, difficili da interpretare. Per questo i ricercatori hanno iniziato a sperimentare nuovi approcci, come toolkit dedicati e modelli di spiegabilità più interattivi, in cui gli utenti non si limitano a ricevere una motivazione, ma possono intervenire sui criteri che producono le raccomandazioni.

Colmare il divario tra principi etici e applicazioni concrete non è un compito secondario, ma la vera sfida del nostro tempo. Non basta sognare un’AI più equa, più trasparente e più sicura: bisogna costruire processi, standard, regolazioni e strumenti che la rendano tale. E questo lavoro non spetta solo agli ingegneri, ma anche ai manager, ai revisori esterni, ai legislatori, ai ricercatori e agli utenti finali. L’HCAI è, in fondo, un progetto collettivo che richiede impegno condiviso, attenzione continua e un principio guida semplice ma essenziale: **la tecnologia deve servire l’essere umano, non sostituirlo né dominarlo.**


## Safety Culture

La sicurezza dei sistemi AI non sia solo una questione di algoritmi, ma soprattutto di organizzazione. I sistemi complessi e interconnessi che caratterizzano la società contemporanea – dalle infrastrutture energetiche alle reti sanitarie, dai trasporti ai servizi digitali – possono causare danni enormi in caso di fallimento. Per questo si sono sviluppate diverse teorie e modelli, come la Normal Accident Theory, le High Reliability Organizations, l’ingegneria della resilienza e le safety cultures. Nonostante differenze di approccio, tutte condividono l’idea che le organizzazioni devono essere preparate al fallimento: studiare i quasi‑incidenti, mantenere ridondanze, distribuire il processo decisionale, promuovere una mentalità orientata alla vigilanza e all’apprendimento.

La safety culture, in particolare, pone al centro il ruolo della leadership, la quale deve impegnarsi apertamente a costruire ambienti sicuri, formare il personale, coinvolgere gli utenti finali e sviluppare procedure solide. Una cultura della sicurezza implica anche una politica sistematica di raccolta di dati su errori e quasi‑incidenti, considerati risorse preziose per imparare e prevenire nuovi problemi. Settori come l’aviazione, il controllo industriale e la sanità offrono già modelli efficaci, come i sistemi di segnalazione FDA, i bug tracker del software o le after‑action reviews dell’esercito statunitense.

Un’altra aspetto cruciale riguarda l’importanza della **certificazione indipendente**, cioè il coinvolgimento di enti esterni – pubblici, privati o normativi – per garantire trasparenza, responsabilità e qualità. Poiché la responsabilità legale rimane sempre in capo agli esseri umani e alle organizzazioni, è fondamentale disporre di audit trail completi, protocolli di indagine, procedure di monitoraggio continuativo e valutazioni strutturate dei rischi. L’Unione Europea, ad esempio, ha già avviato percorsi per chiarire la responsabilità nei casi di danni causati da AI, mentre negli Stati Uniti enti come la SEC o le commissioni di sicurezza nel settore dei trasporti rappresentano modelli consolidati. Anche il settore privato si sta muovendo in questa direzione: aziende di consulenza, società di audit e perfino compagnie assicurative stanno definendo standard e criteri per valutare affidabilità e rischio dei sistemi AI. L'idea di un “building code for software” – analogia con le norme edilizie – mostra bene la direzione auspicata: norme chiare, verificabili e universalmente applicabili.

Realtà come l’Algorithmic Justice League hanno mostrato la loro capacità di influenzare il settore, spingendo aziende globali a ridurre bias e ingiustizie nei loro sistemi. Tuttavia, il potere delle ONG rimane limitato: possono denunciare problemi, promuovere discussioni e proporre soluzioni, ma non hanno autorità diretta. Per questo è necessario creare sinergie tra ONG, governi, aziende, enti di standardizzazione e istituti di ricerca. Le società professionali – come IEEE o ACM – hanno già iniziato a sviluppare standard e linee guida etiche, ma serve una partecipazione più ampia e sistematica dei professionisti del settore.

Le **politiche pubbliche** stiano modellando un ecosistema globale dell’AI molto eterogeneo. La Cina, ad esempio, punta a diventare leader mondiale entro il 2030 e adotta un approccio in cui il beneficio collettivo giustifica un’ampia raccolta di dati personali. Gli Stati Uniti privilegiano una strategia di investimento nella ricerca collaborativa, mentre l’Europa insiste su principi quali la trasparenza, la protezione dei dati, l’equità e la supervisione umana. A livello globale, iniziative come quelle dell’OECD o dell’ONU provano a costruire cornici comuni, promuovendo un uso responsabile dell’AI per il bene pubblico e lo sviluppo sostenibile.

## Conclusione

La costruzione di sistemi HCAI richiede quattro livelli di governance – sistemi affidabili, cultura della sicurezza, certificazione indipendente e regolamentazione governativa – che lavorano insieme per tradurre i principi etici in pratiche operative. Tuttavia, avverte anche che i rischi esistono: bias, privacy violata, malfunzionamenti e uso malevolo dell’AI sono minacce concrete. Ma con una governance intelligente, una progettazione centrata sull’uomo e un impegno congiunto di attori pubblici e privati, è possibile costruire un futuro tecnologico che rispetti dignità, diritti e valori umani.