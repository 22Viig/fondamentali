---
layout: page
title: Human-Centred Artificial Intelligence 3
args: (Recommender System, Expleinability, Etica, Responsible Ai)
permalink: /HCAI-03/

---

Argomenti
- [1. Recommender System](#Recommender-System)
- [2. Expleinability](#Expleinability)
- [3. Etica](#Etica)
- [4. Responsible Ai](#Responsible-Ai)
- [5. Conclusioni](#Conclusioni)


I sistemi di raccomandazione si distinguano dai classici sistemi di Information Retrieval: mentre questi ultimi rispondono a una richiesta esplicita dell’utente e devono recuperare tutti i documenti rilevanti, i recommender systems operano in modo proattivo. Non attendono una query formale, ma analizzano preferenze, cronologia e contesto per suggerire contenuti rilevanti. Questo passaggio da un modello pull a uno push caratterizza l’intero settore: l’obiettivo non è trovare ciò che l’utente cerca, ma **scoprire ciò che potrebbe piacergli**. È qui che entrano in gioco la personalizzazione, l’uso di informazioni della comunità (come comportamenti simili di altri utenti) e la capacità di riconoscere preferenze implicite, come clic, acquisti o visualizzazioni.

Il compito principale di un sistema di raccomandazione può assumere due forme: la **Top‑N recommendation**, che punta a selezionare un insieme ristretto di elementi rilevanti, e la **rating prediction**, orientata invece a stimare il grado di apprezzamento di un utente per un certo oggetto. Sebbene oggi prevalgano gli approcci Top‑N, anche la capacità di prevedere un rating rimane utile in applicazioni come film, prodotti o recensioni.

Esistono diverse tecniche di raccomandazione, raggruppabili in tre macro‑famiglie: metodi content‑based, collaborative filtering e soluzioni ibride. 

Le strategie **content‑based** analizzano direttamente le caratteristiche degli oggetti, come parole nei testi, metadati, tag, o feature audiovisive. Per valutare la similarità fra documenti testuali, ad esempio, si utilizza spesso il modello vettoriale: un documento è trattato come un vettore di termini, ponderati attraverso tecniche come tf–idf. Il processo prevede la rimozione delle stop‑words, la normalizzazione e l’applicazione dello stemming per ricondurre le parole alla loro radice. Il risultato è uno spazio vettoriale in cui la distanza o l’angolo tra vettori rappresenta la similarità semantica tra gli oggetti.

In contrasto, il **collaborative filtering** ignora il contenuto degli item e analizza esclusivamente lo schema di valutazioni degli utenti. La logica è che due elementi sono simili se le stesse persone li apprezzano o li rifiutano. Due utenti sono simili se hanno gusti coerenti. Questa famiglia di metodi è particolarmente potente quando le caratteristiche degli oggetti sono difficili da estrarre (ad esempio, immagini, video, audio). Tuttavia, può soffrire della cosiddetta “cold start problem” per utenti o item nuovi, dato che richiede un numero sufficiente di interazioni pregresse per funzionare bene. Un esempio classico di collaborative filtering è il nearest‑neighbor, che usa la vicinanza fra utenti o item per calcolare una previsione. L’algoritmo analizza quali altri utenti simili hanno valutato un certo oggetto e deduce di conseguenza la probabile valutazione dell’utente corrente.

I sistemi di raccomandazione possono avere approcci user‑based e item‑based. Nel primo caso si calcolano somiglianze tra gli utenti e si deduce il rating di un nuovo item aggregando i giudizi degli utenti più affini. Nell’approccio item‑based si assumono invece relazioni di similarità fra gli oggetti. Questa seconda modalità è stata storicamente preferita da aziende come Amazon, perché gli item sono generalmente più stabili nel tempo rispetto agli utenti, le cui preferenze possono essere molto dinamiche.

Esistono tecniche ibride, che combinano contenuto e collaborazione per mitigare i limiti di entrambe le strategie. La fusione può avvenire in diversi modi: votazioni per aggregare risultati, medie pesate o modelli personalizzati che combinano le due fonti di informazione. Gli approcci ibridi risultano spesso più robusti, soprattutto in domini complessi dove sia i dati testuali sia il comportamento degli utenti forniscono segnali significativi.

C'è una sostanziale distinzione tra metodi **memory‑based** e **model‑based**. Gli approcci memory‑based utilizzano direttamente la matrice delle valutazioni, senza costruire modelli astratti: la similarità è calcolata al momento, basandosi sulle interazioni correnti. Sono semplici e sempre aggiornati, ma poco scalabili e sensibili alla sparsità. I model‑based costruiscono invece un modello latente, come ad esempio con la decomposizione in valori singolari (SVD). Questo permette di identificare dimensioni nascoste che rappresentano gusti o caratteristiche implicite. I vantaggi sono consistenza, riduzione del rumore e rapidità nelle raccomandazioni; lo svantaggio è la necessità di aggiornare periodicamente il modello.

In base al tipo di task, cambiano le metriche. Per i Top‑N, la **precisione** e il **recall** misurano rispettivamente quanti suggerimenti sono corretti e quanti elementi rilevanti sono stati trovati. L’**F‑measur**e combina le due metriche per dare un valore unificato. Quando il ranking conta, si utilizzano misure come la **Mean Average Precision** (MAP), che considera la posizione dei risultati. La ROC curve, con l’AUC, è un'altra tecnica che visualizza il compromesso tra veri positivi e falsi positivi lungo diverse soglie. Infine, per i sistemi di rating prediction, è comune il ricorso alla **Root Mean Squared Error** (RMSE), che misura lo scarto medio tra valutazioni reali e previste.

## Expleinability

Gli utenti non sempre comprendono perché un modello prenda una decisione, e in mancanza di trasparenza possono perdere fiducia nel sistema. Attraverso esempi visivi – come immagini classificate erroneamente o casi clinici in cui una previsione non è giustificata – si mette in evidenza un problema strutturale: un algoritmo può restituire la risposta giusta per le ragioni sbagliate. I casi celebri di bias razziale in sistemi come Google Photos e Google Vision API dimostrano quanto sia facile che un modello apprenda associazioni distorte presenti nei dati, portando a risultati inaccettabili. In queste circostanze, semplici correzioni come filtrare categorie o nascondere risultati non risolvono il problema, ma lo occultano.

A questo punto diventa chiaro che l'**Explainable Artificial Intelligence** (XAI) non è un lusso, ma una componente fondamentale di sistemi sicuri e affidabili. La XAI è il campo dell’intelligenza artificiale dedicato a rendere interpretabili i modelli complessi, in modo sistematico e human‑interpretable. Inserita nel ciclo di vita di un progetto di machine learning, la XAI cambia radicalmente la modalità con cui i modelli vengono progettati, analizzati e valutati. Il passaggio dal paradigma tradizionale – in cui il modello fornisce output senza spiegazioni – a un modello spiegabile implica che l’utente possa comprendere perché un modello ha avuto successo o fallito, e quando è opportuno fidarsi delle sue predizioni.

Nel panorama dell’interpretabilità vengono introdotte due grandi categorie: metodi model‑specific e metodi model‑agnostic. I primi funzionano solo con determinate classi di modelli, sfruttando la loro struttura interna (ad esempio DeepLIFT per reti neurali). I secondi invece operano come una sorta di “lente esterna”: trattano il modello come una black box e analizzano solo input e output. Fra questi, il documento approfondisce LIME (Local Interpretable Model‑agnostic Explanations), una delle tecniche più diffuse. LIME cerca un modello interpretabile che approssimi il comportamento del modello complesso in una piccola regione locale attorno all’istanza da spiegare. Tale modello locale viene costruito perturbando l’input, valutando le predizioni del modello black box sui punti perturbati, pesando le osservazioni in base alla vicinanza e infine addestrando un modello interpretabile (ad esempio una regressione lineare o un albero semplice). LIME, dunque, non spiega l’intero modello, ma fornisce una spiegazione locale che permette all’utente di capire quali feature hanno influenzato la predizione in quel caso specifico.

Il modello SHAP, basato sui valori di Shapley della teoria dei giochi, fornisce un insieme di proprietà teoriche – local accuracy, missingness e consistency – che garantiscono che la spiegazione sia coerente. SHAP può essere visto come un generalizzatore di molti metodi precedenti, compreso LIME, e per questo è oggi molto usato in ambito industriale.

Anchors è un algoritmo che cerca di costruire regole di tipo “if‑then” che ancorano la predizione: se una regola è un anchor, significa che cambiando molte altre variabili la decisione del modello non cambia. Questo approccio permette spiegazioni più stabili e intuitive, soprattutto in contesti in cui è utile rappresentare la logica del modello in termini di condizioni discrete e facilmente comunicabili a esseri umani.

L’apparente successo di un modello – ad esempio un classificatore di immagini con un’accuratezza dell’83% – può nascondere dinamiche problematiche. L’esempio dei lupi e degli husky dimostra che un modello può classificare correttamente molti casi pur basandosi su una feature irrilevante, come la presenza della neve nello sfondo. Non è dunque il livello di accuratezza a garantire l’affidabilità del modello, ma la comprensione dei fattori che guidano le predizioni. **È questo il cuore dell’Explainable AI: non accontentarsi che un modello funzioni, ma verificare che funzioni per le ragioni giuste.** 

L’esigenza di personalizzare suggerimenti, massimizzare valore economico e al tempo stesso garantire equità, correttezza e trasparenza richiede un approccio integrato. La XAI rappresenta lo strumento che consente di rendere più responsabile l’uso dei modelli complessi, assicurando che le decisioni automatiche non solo siano performanti, ma anche comprensibili e accettabili dal punto di vista sociale, etico e operativo.

## Etica

La **ethical decision-making** nell’AI è capacità di un sistema di valutare e scegliere tra alternative in modo coerente con norme sociali, etiche e legali. Questo implica che l’AI debba riconoscere le opzioni non etiche, scartarle e selezionare quella che, pur consentendo il raggiungimento di un obiettivo, rispetta l’azione più giusta o più “benefica” secondo i valori della società. 

Da ciò derivano i criteri fondamentali di un’azione etica: il sistema deve avere una reale possibilità di scelta, deve essere in grado di identificare tra le alternative quella considerata socialmente buona e deve farlo consapevolmente, cioè proprio perché riconosce tale scelta come etica. Questo già suggerisce la complessità del problema: mentre la legge può essere imposta dall’esterno, l’etica richiede interiorizzazione, apprendimento, sensibilità al contesto. Per questo il documento sottolinea che l’etica non può essere semplicemente “programmata” come una serie di regole assolute.

Considerando le teorie morali, gli esseri umani combinano differenti visioni: talvolta ragioniamo in base alle conseguenze (visione utilitaristica), talvolta in base ai principi (visione deontologica), altre volte in base alle virtù e al carattere (virtue ethics). Non seguiamo mai rigidamente un’unica teoria, e proprio questa flessibilità ci permette di evitare risultati paradossali e di adattarci a contesti complessi. Rendere computazionale una capacità così sfumata è estremamente difficile, esistono tre approcci mediante i quali si cerca di far ragionare eticamente un sistema di AI: top‑down, bottom‑up e approcci ibridi.

L’approccio **top‑down** parte da una teoria etica predefinita, trasformata in regole o principi che il sistema deve seguire. È un metodo che presuppone che l’AI sia in grado di ragionare esplicitamente sulle conseguenze delle proprie azioni, valutando ciò che è moralmente corretto sulla base di una rappresentazione formale di valori, norme e conoscenze del dominio. Questo approccio funziona bene in sistemi in cui è importante garantire coerenza e verificabilità, ma presenta due limiti: da un lato riduce la complessità dell’etica a un insieme rigido di regole, dall’altro rischia di confondere etica e legalità, assumendo che ciò che è conforme alla legge sia automaticamente etico.

L’approccio **bottom‑up**, invece, si basa sull’idea che il comportamento etico possa essere appreso dall’osservazione dei comportamenti umani, un po’ come accade nei modelli di machine learning tradizionali. La “moralità” è derivata dal consenso sociale, da ciò che la maggior parte delle persone ritiene giusto in un determinato contesto. Questo metodo però introduce nuove sfide: ciò che è considerato accettabile varia da cultura a cultura, e ciò che la società accetta non sempre coincide con ciò che è moralmente corretto. Anche dal punto di vista concettuale, basarsi solo sulla “forza della maggioranza” è rischioso, perché può rinforzare pregiudizi e discriminazioni già presenti.

Da qui nasce la necessità degli approcci ibridi, che combinano la stabilità dei principi top‑down con la sensibilità contestuale del bottom‑up. L’approccio MOOD si fonda sulla “**intelligenza collettiva**” e cerca di integrare molteplici prospettive, evitando allo stesso tempo il dominio della maggioranza. Le decisioni etiche, infatti, implicano anche riflessioni sulla distribuzione dei benefici e dei rischi, sulla possibile creazione di danni futuri, su potenziali situazioni di oppressione o squilibri di potere.

Nella ricerca dei così detti **artificial moral agents**, ossia sistemi capaci di prendere decisioni moralmente informate, è importante la partecipazione il coinvolgimento degli stakeholder nei processi decisionali. Per costruire sistemi morali serve una riflessione accurata su elementi come la composizione della “folla” da cui si apprende, la formulazione delle domande poste agli utenti, la modalità con cui questi vengono informati e coinvolti, il sistema elettorale adottato nei processi partecipativi e la legittimità dei risultati. L’autore richiama le caratteristiche della deliberazione democratica delineate da Fishkin, come la necessità di fornire informazioni accurate, confrontare posizioni diverse, garantire diversità e promuovere discussioni sincere e consapevoli.

L’etica di un agente AI può essere implementata nei sistemi autonomi per mezzo di un approccio strettamente algoritmico, in cui il sistema incorpora meccanismi per valutare autonomamente le conseguenze morali, a soluzioni che prevedono un human in command, cioè un essere umano che supervisiona, controlla o affianca la decisione del sistema. Seguono poi modelli basati sulla regolamentazione dell’ambiente – che impedisce all’AI di trovarsi in situazioni moralmente ambigue –, fino alla considerazione che, in casi estremi, un sistema potrebbe persino adottare una scelta casuale quando nessuna decisione è eticamente preferibile.

Nessun artefatto, per quanto avanzato, può essere considerato autonomo nel senso filosofico del termine, poiché non possiede volontà, intenzionalità o capacità normativa. Perciò non può essere equiparato a un essere umano né può avere una sua dignità morale. **L’etica resta sempre responsabilità dell'essere umano che progetta, controlla o utilizza il sistema.** 

## Responsible Ai

Responsible AI, questa espressione può assumere molteplici significati: dalle politiche di governance alla responsabilità degli sviluppatori, dalla promozione dell’inclusione alla riflessione sui rischi e sui benefici dell’AI. Dall' IEEE alle piattaforme dell’Unione Europea, dalle partnership industriali alle strategie nazionali sono alcune tra le principali iniziative internazionali, tutte accomunate dall’obiettivo di garantire che lo sviluppo dell’AI sia allineato a valori come sicurezza, trasparenza, equità e responsabilità.

Da un lato si teme che regole troppo restrittive possano rallentare l’innovazione; dall’altro, si riconosce che senza regole l’AI rischia di svilupparsi in modo opaco e potenzialmente dannoso. Una regolamentazione ben progettata può stimolare lo sviluppo scientifico, imponendo standard elevati che favoriscono soluzioni più robuste, interpretabili e affidabili. Da qui deriva anche l’idea della certificazione, come nel programma IEEE ECPAIS, pensato per garantire la qualità dei sistemi attraverso controlli indipendenti e standard condivisi.

Un altro pilastro della Responsible AI è rappresentato da**i codici di condotta professionali**, come quello dell’ACM, che stabiliscono linee guida affinché i professionisti dell’informatica agiscano in modo eticamente responsabile. Tali codici aiutano a orientare le decisioni, gestire dilemmi e comunicare alla società gli impegni assunti dalla professione.

In termini di **inclusione e diversità** lo sviluppo di sistemi di AI ha impatti profondi su società plurali e culturalmente eterogenee. Ignorare diversità culturali, religiose, linguistiche significa rischiare di escludere intere fasce di popolazione dai benefici dell’innovazione. In parallelo, l’autore analizza il “narrative frame” contemporaneo sull’AI, richiamando l'attenzione sulle rappresentazioni antropomorfiche e sugli effetti che queste possono avere sulla percezione sociale dei sistemi intelligenti. Proprio perché l’AI sta assumendo forme sempre più simili a interfacce umane – come assistenti vocali o robot conversazionali –, è fondamentale interrogarsi sull’uso responsabile di tali scelte estetiche e funzionali, evitando suggestioni ingannevoli e rispettando la dignità degli utenti.

## Conclusioni

 L’etica dell’AI e la Responsible AI non siano campi astratti o aggiuntivi, ma dimensioni centrali nel design e nella diffusione delle tecnologie intelligenti. Solo integrando principi morali, coinvolgimento democratico, regolamentazione adeguata, inclusione e trasparenza è possibile costruire sistemi che non solo funzionino in modo efficace, ma rispettino e promuovano i valori fondamentali della società.